{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4766b813",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-09T05:01:25.976208Z",
     "iopub.status.busy": "2024-10-09T05:01:25.975176Z",
     "iopub.status.idle": "2024-10-09T05:01:25.981400Z",
     "shell.execute_reply": "2024-10-09T05:01:25.980479Z"
    },
    "papermill": {
     "duration": 0.016538,
     "end_time": "2024-10-09T05:01:25.983752",
     "exception": false,
     "start_time": "2024-10-09T05:01:25.967214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a486c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:01:25.995910Z",
     "iopub.status.busy": "2024-10-09T05:01:25.995019Z",
     "iopub.status.idle": "2024-10-09T05:02:29.947077Z",
     "shell.execute_reply": "2024-10-09T05:02:29.946061Z"
    },
    "papermill": {
     "duration": 63.960925,
     "end_time": "2024-10-09T05:02:29.949930",
     "exception": false,
     "start_time": "2024-10-09T05:01:25.989005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\r\n",
      "remote: Enumerating objects: 233525, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (20936/20936), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (1499/1499), done.\u001b[K\r\n",
      "remote: Total 233525 (delta 20370), reused 19548 (delta 19343), pack-reused 212589 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (233525/233525), 239.33 MiB | 26.90 MiB/s, done.\r\n",
      "Resolving deltas: 100% (171022/171022), done.\r\n",
      "Collecting av\r\n",
      "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\r\n",
      "Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: av\r\n",
      "Successfully installed av-13.1.0\r\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/huggingface/transformers.git\n",
    "! pip3 install av\n",
    "    \n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import av\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import VivitConfig, VivitModel, VivitImageProcessor, VivitForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e55a7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.005056Z",
     "iopub.status.busy": "2024-10-09T05:02:30.004272Z",
     "iopub.status.idle": "2024-10-09T05:02:30.127073Z",
     "shell.execute_reply": "2024-10-09T05:02:30.126125Z"
    },
    "papermill": {
     "duration": 0.152646,
     "end_time": "2024-10-09T05:02:30.129781",
     "exception": false,
     "start_time": "2024-10-09T05:02:29.977135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '../input/hand-wash-dataset/HandWashDataset/HandWashDataset'\n",
    "\n",
    "classes = ['Step_1', 'Step_2_Left', 'Step_2_Right', 'Step_3', 'Step_4_Left', 'Step_4_Right', 'Step_5_Left',\n",
    "           'Step_5_Right', 'Step_6_Left', 'Step_6_Right', 'Step_7_Left', 'Step_7_Right']\n",
    "\n",
    "def get_classes_filenames(data_dir, classes):\n",
    "    \"\"\"\n",
    "    Filenames of files in data_dir/classes[i] for each i\n",
    "    Args:\n",
    "        data_dir (`str`): filepath to a directory\n",
    "        classes (`list[str]`): list of subdirectories of data_dir\n",
    "    Return:\n",
    "        filenames (`dict`): dictionary of filenames\n",
    "    \"\"\"\n",
    "    filenames = {}\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        filenames[class_name] = os.listdir(class_dir)\n",
    "    return filenames\n",
    "\n",
    "filenames = get_classes_filenames(data_dir, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c372f589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.182244Z",
     "iopub.status.busy": "2024-10-09T05:02:30.181766Z",
     "iopub.status.idle": "2024-10-09T05:02:30.199372Z",
     "shell.execute_reply": "2024-10-09T05:02:30.198282Z"
    },
    "papermill": {
     "duration": 0.047058,
     "end_time": "2024-10-09T05:02:30.201782",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.154724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxilliary functions from Hugging Face\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "label2id = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "id2label = {idx : class_name for idx, class_name in enumerate(classes)}\n",
    "\n",
    "# Class of dataset\n",
    "class HandwashingDataset(Dataset):\n",
    "    \"\"\"Handwashing Dataset.\"\"\"\n",
    "    def __init__(self, file_path, file_names, image_processor, transform=None):\n",
    "        self.labels = []\n",
    "        self.file_paths = []\n",
    "        self.image_processor = image_processor\n",
    "        for label in file_names.keys():\n",
    "            for name in file_names[label]:\n",
    "                full_path = os.path.join(file_path, label, name)\n",
    "                self.file_paths.append(full_path)\n",
    "                self.labels.append(label2id[label])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_container = av.open(self.file_paths[idx])\n",
    "        vid_indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=vid_container.streams.video[0].frames)\n",
    "        vid_read = read_video_pyav(container=vid_container, indices=vid_indices)\n",
    "        vid_input = self.image_processor(list(vid_read), return_tensors=\"pt\")\n",
    "        vid_input['pixel_values'] = vid_input['pixel_values'].squeeze(0)\n",
    "        \n",
    "        return {'video' : vid_input, 'class' : torch.tensor(self.labels[idx], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecac21d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.250433Z",
     "iopub.status.busy": "2024-10-09T05:02:30.249649Z",
     "iopub.status.idle": "2024-10-09T05:02:30.264996Z",
     "shell.execute_reply": "2024-10-09T05:02:30.264042Z"
    },
    "papermill": {
     "duration": 0.042258,
     "end_time": "2024-10-09T05:02:30.267757",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.225499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(filenames, test_size=0.2, random_state=20):\n",
    "    test = {}\n",
    "    train = {}\n",
    "    for label in filenames.keys():\n",
    "        train_names, test_names = train_test_split(filenames[label], test_size=test_size, random_state=random_state)\n",
    "        test[label] = test_names\n",
    "        train[label] = train_names\n",
    "    return train, test\n",
    "\n",
    "# Preparing the dataset and splititng into training and test datasets\n",
    "train_vid_names, test_vid_names = split_dataset(filenames, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20407eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.317786Z",
     "iopub.status.busy": "2024-10-09T05:02:30.317378Z",
     "iopub.status.idle": "2024-10-09T05:02:30.326692Z",
     "shell.execute_reply": "2024-10-09T05:02:30.325512Z"
    },
    "papermill": {
     "duration": 0.03804,
     "end_time": "2024-10-09T05:02:30.329419",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.291379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimage_processor = VivitImageProcessor(rescale_factor=1/255.0,offset=False,size={\"height\": 224, \"width\": 224})\\nprint(image_processor.image_mean)\\ndataset = HandwashingDataset(data_dir, train_vid_names, image_processor)\\nitem = dataset[0]\\nlabel = item[\\'class\\']\\nvideo = item[\\'video\\']\\nprint(video[\\'pixel_values\\'].shape)\\n\\nvid_container = av.open(dataset.file_paths[0])\\nvid_indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=vid_container.streams.video[0].frames)\\nvid_read = read_video_pyav(container=vid_container, indices=vid_indices)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Messing around with dataset, remove when pushing to git\n",
    "\"\"\"\n",
    "image_processor = VivitImageProcessor(rescale_factor=1/255.0,offset=False,size={\"height\": 224, \"width\": 224})\n",
    "print(image_processor.image_mean)\n",
    "dataset = HandwashingDataset(data_dir, train_vid_names, image_processor)\n",
    "item = dataset[0]\n",
    "label = item['class']\n",
    "video = item['video']\n",
    "print(video['pixel_values'].shape)\n",
    "\n",
    "vid_container = av.open(dataset.file_paths[0])\n",
    "vid_indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=vid_container.streams.video[0].frames)\n",
    "vid_read = read_video_pyav(container=vid_container, indices=vid_indices)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27e9e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.382134Z",
     "iopub.status.busy": "2024-10-09T05:02:30.381654Z",
     "iopub.status.idle": "2024-10-09T05:02:30.389284Z",
     "shell.execute_reply": "2024-10-09T05:02:30.388166Z"
    },
    "papermill": {
     "duration": 0.036787,
     "end_time": "2024-10-09T05:02:30.391690",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.354903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport cv2\\nre_shifted = video['pixel_values'].numpy() * 255.0\\n\\ndef resize_video(video_array, target_size=(224, 224)):\\n    num_frames, height, width, channels = video_array.shape\\n    resized_video = np.zeros((num_frames, target_size[0], target_size[1], channels), dtype=video_array.dtype)\\n    for i in range(num_frames):\\n        frame = video_array[i]\\n        resized_frame = cv2.resize(frame, target_size)\\n        resized_video[i] = resized_frame\\n    resized_video = np.transpose(resized_video, (0, 3, 1, 2))\\n    return resized_video\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import cv2\n",
    "re_shifted = video['pixel_values'].numpy() * 255.0\n",
    "\n",
    "def resize_video(video_array, target_size=(224, 224)):\n",
    "    num_frames, height, width, channels = video_array.shape\n",
    "    resized_video = np.zeros((num_frames, target_size[0], target_size[1], channels), dtype=video_array.dtype)\n",
    "    for i in range(num_frames):\n",
    "        frame = video_array[i]\n",
    "        resized_frame = cv2.resize(frame, target_size)\n",
    "        resized_video[i] = resized_frame\n",
    "    resized_video = np.transpose(resized_video, (0, 3, 1, 2))\n",
    "    return resized_video\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fc4d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.444476Z",
     "iopub.status.busy": "2024-10-09T05:02:30.443339Z",
     "iopub.status.idle": "2024-10-09T05:02:30.455081Z",
     "shell.execute_reply": "2024-10-09T05:02:30.454017Z"
    },
    "papermill": {
     "duration": 0.04065,
     "end_time": "2024-10-09T05:02:30.457511",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.416861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_vivit_model(model, data_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the Vision Transformer (ViViT) model.\n",
    "    \n",
    "    Args:\n",
    "        model (`nn.Module`): The ViViT model to train.\n",
    "        data_loader (`DataLoader`): The PyTorch DataLoader with HandwashingDataset.\n",
    "        criterion (`torch.nn.Module`): Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (`torch.optim.Optimizer`): Optimizer (e.g., Adam, SGD).\n",
    "        device (`torch.device`): Device to train on ('cuda' or 'cpu').\n",
    "        num_epochs (`int`): Number of epochs to train (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained ViViT model.\n",
    "    \"\"\"\n",
    "    # Move model to the device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Loop over data batches\n",
    "        for batch in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Get inputs and labels, move them to the device\n",
    "            inputs = batch['video'].to(device)  # (batch_size, num_frames, C, H, W)\n",
    "            labels = batch['class'].to(device)  # (batch_size,)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)  # (batch_size, num_classes)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            # Backward pass + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.logits.argmax(dim=-1)  # Get predicted class\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = correct_predictions / total_samples\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ac3947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.510889Z",
     "iopub.status.busy": "2024-10-09T05:02:30.510449Z",
     "iopub.status.idle": "2024-10-09T05:02:30.517144Z",
     "shell.execute_reply": "2024-10-09T05:02:30.516014Z"
    },
    "papermill": {
     "duration": 0.036778,
     "end_time": "2024-10-09T05:02:30.519543",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.482765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shrink the size of the model\n",
    "# config = VivitConfig(hidden_size=256,num_hidden_layers=4,intermediate_size=768,num_attention_heads=4)\n",
    "# make the model large again\n",
    "config = VivitConfig()\n",
    "\n",
    "config.num_labels = len(classes)\n",
    "for i in range(len(classes)):\n",
    "    id2label[i] = classes[i]\n",
    "    label2id[classes[i]] = i\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6eeb1c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T05:02:30.574121Z",
     "iopub.status.busy": "2024-10-09T05:02:30.573572Z",
     "iopub.status.idle": "2024-10-09T09:38:16.454078Z",
     "shell.execute_reply": "2024-10-09T09:38:16.452443Z"
    },
    "papermill": {
     "duration": 16545.911144,
     "end_time": "2024-10-09T09:38:16.456525",
     "exception": false,
     "start_time": "2024-10-09T05:02:30.545381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(value)\n",
      "Epoch 1/20: 100%|██████████| 240/240 [13:47<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.4229, Accuracy: 0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 240/240 [13:52<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 2.9216, Accuracy: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 240/240 [13:37<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 2.7797, Accuracy: 0.0917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 240/240 [13:53<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 2.7523, Accuracy: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 240/240 [13:29<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 2.6538, Accuracy: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 240/240 [13:46<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 2.6369, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 240/240 [13:38<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 2.5629, Accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 240/240 [13:59<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 2.5771, Accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 240/240 [13:53<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 2.5718, Accuracy: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 240/240 [14:00<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 2.5580, Accuracy: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 240/240 [13:52<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 2.5562, Accuracy: 0.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 240/240 [13:45<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 2.5466, Accuracy: 0.0917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 240/240 [14:02<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 2.5530, Accuracy: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 240/240 [13:49<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 2.5465, Accuracy: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 240/240 [13:44<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 2.5318, Accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 240/240 [13:49<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 2.5226, Accuracy: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 240/240 [13:35<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 2.5313, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 240/240 [13:45<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 2.5252, Accuracy: 0.0917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 240/240 [13:39<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 2.5299, Accuracy: 0.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 240/240 [13:39<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 2.5173, Accuracy: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VivitForVideoClassification(\n",
       "  (vivit): VivitModel(\n",
       "    (embeddings): VivitEmbeddings(\n",
       "      (patch_embeddings): VivitTubeletEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): VivitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VivitLayer(\n",
       "          (attention): VivitAttention(\n",
       "            (attention): VivitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VivitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VivitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_act_fn): FastGELUActivation()\n",
       "          )\n",
       "          (output): VivitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model from the config\n",
    "model = VivitForVideoClassification(config)\n",
    "# model = torch.load(\"cur_model.pkl\", weights_only=False)\n",
    "# model.load_state_dict(torch.load(\"cur_model_weights.pkl\", weights_only=True))\n",
    "\n",
    "# set the optimizer, criterion, and device\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available else nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# input the data\n",
    "# image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "image_processor = VivitImageProcessor(rescale_factor=1/255.0,offset=False,size={\"height\": 224, \"width\": 224})\n",
    "dataset = HandwashingDataset(data_dir, train_vid_names, image_processor)\n",
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "train_vivit_model(model, data_loader, criterion, optimizer, device, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d361fc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T09:38:17.472338Z",
     "iopub.status.busy": "2024-10-09T09:38:17.471866Z",
     "iopub.status.idle": "2024-10-09T09:38:18.044363Z",
     "shell.execute_reply": "2024-10-09T09:38:18.043016Z"
    },
    "papermill": {
     "duration": 1.050364,
     "end_time": "2024-10-09T09:38:18.047433",
     "exception": false,
     "start_time": "2024-10-09T09:38:16.997069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model, \"cur_model.pkl\")\n",
    "torch.save(model.state_dict(), \"cur_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaffaaa",
   "metadata": {
    "papermill": {
     "duration": 0.477512,
     "end_time": "2024-10-09T09:38:19.009021",
     "exception": false,
     "start_time": "2024-10-09T09:38:18.531509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"cur_model.pkl\"> Download File </a>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 416449,
     "sourceId": 1093962,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16621.311085,
   "end_time": "2024-10-09T09:38:23.922857",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-09T05:01:22.611772",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
