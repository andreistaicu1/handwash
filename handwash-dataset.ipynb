{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1093962,"sourceType":"datasetVersion","datasetId":416449}],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T02:37:56.645041Z","iopub.execute_input":"2024-10-08T02:37:56.645312Z","iopub.status.idle":"2024-10-08T02:37:56.650295Z","shell.execute_reply.started":"2024-10-08T02:37:56.645277Z","shell.execute_reply":"2024-10-08T02:37:56.649467Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! git clone https://github.com/huggingface/transformers.git\n! pip3 install av\n    \nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn as nn\nimport shutil\nimport av\nimport sys\nimport torch\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import VivitConfig, VivitModel, VivitImageProcessor, VivitForVideoClassification","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:37:56.670655Z","iopub.execute_input":"2024-10-08T02:37:56.671124Z","iopub.status.idle":"2024-10-08T02:38:48.687174Z","shell.execute_reply.started":"2024-10-08T02:37:56.671091Z","shell.execute_reply":"2024-10-08T02:38:48.686330Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'transformers'...\nremote: Enumerating objects: 233241, done.\u001b[K\nremote: Counting objects: 100% (594/594), done.\u001b[K\nremote: Compressing objects: 100% (336/336), done.\u001b[K\nremote: Total 233241 (delta 367), reused 384 (delta 211), pack-reused 232647 (from 1)\u001b[K\nReceiving objects: 100% (233241/233241), 247.17 MiB | 27.26 MiB/s, done.\nResolving deltas: 100% (169932/169932), done.\nCollecting av\n  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-13.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir = '../input/hand-wash-dataset/HandWashDataset/HandWashDataset'\n\nclasses = ['Step_1', 'Step_2_Left', 'Step_2_Right', 'Step_3', 'Step_4_Left', 'Step_4_Right', 'Step_5_Left',\n           'Step_5_Right', 'Step_6_Left', 'Step_6_Right', 'Step_7_Left', 'Step_7_Right']\n\ndef get_classes_filenames(data_dir, classes):\n    \"\"\"\n    Filenames of files in data_dir/classes[i] for each i\n    Args:\n        data_dir (`str`): filepath to a directory\n        classes (`list[str]`): list of subdirectories of data_dir\n    Return:\n        filenames (`dict`): dictionary of filenames\n    \"\"\"\n    filenames = {}\n    for class_name in classes:\n        class_dir = os.path.join(data_dir, class_name)\n        filenames[class_name] = os.listdir(class_dir)\n    return filenames\n\nfilenames = get_classes_filenames(data_dir, classes)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:38:48.688833Z","iopub.execute_input":"2024-10-08T02:38:48.689439Z","iopub.status.idle":"2024-10-08T02:38:48.806999Z","shell.execute_reply.started":"2024-10-08T02:38:48.689381Z","shell.execute_reply":"2024-10-08T02:38:48.806170Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Auxilliary functions from Hugging Face\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices\n\nlabel2id = {class_name: idx for idx, class_name in enumerate(classes)}\nid2label = {idx : class_name for idx, class_name in enumerate(classes)}\n\n# Class of dataset\nclass HandwashingDataset(Dataset):\n    \"\"\"Handwashing Dataset.\"\"\"\n    def __init__(self, file_path, file_names, image_processor, transform=None):\n        self.labels = []\n        self.file_paths = []\n        self.image_processor = image_processor\n        for label in file_names.keys():\n            for name in file_names[label]:\n                full_path = os.path.join(file_path, label, name)\n                self.file_paths.append(full_path)\n                self.labels.append(label2id[label])\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        vid_container = av.open(self.file_paths[idx])\n        vid_indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=vid_container.streams.video[0].frames)\n        vid_read = read_video_pyav(container=vid_container, indices=vid_indices)\n        vid_input = self.image_processor(list(vid_read), return_tensors=\"pt\")\n        \n        return {'video' : vid_input, 'class' : torch.tensor([self.labels[idx]], dtype=torch.long)}","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:38:48.808169Z","iopub.execute_input":"2024-10-08T02:38:48.808459Z","iopub.status.idle":"2024-10-08T02:38:48.822827Z","shell.execute_reply.started":"2024-10-08T02:38:48.808429Z","shell.execute_reply":"2024-10-08T02:38:48.821902Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def split_dataset(filenames, test_size=0.2, random_state=20):\n    test = {}\n    train = {}\n    for label in filenames.keys():\n        train_names, test_names = train_test_split(filenames[label], test_size=test_size, random_state=random_state)\n        test[label] = test_names\n        train[label] = train_names\n    return train, test\n\n# Preparing the dataset and splititng into training and test datasets\ntrain_vid_names, test_vid_names = split_dataset(filenames, random_state=8)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:38:48.824964Z","iopub.execute_input":"2024-10-08T02:38:48.825298Z","iopub.status.idle":"2024-10-08T02:38:48.844724Z","shell.execute_reply.started":"2024-10-08T02:38:48.825266Z","shell.execute_reply":"2024-10-08T02:38:48.843893Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"config = VivitConfig()\n\nconfig.num_labels = len(classes)\nfor i in range(len(classes)):\n    id2label[i] = classes[i]\n    label2id[classes[i]] = i\nconfig.id2label = id2label\nconfig.label2id = label2id","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:38:48.845709Z","iopub.execute_input":"2024-10-08T02:38:48.845976Z","iopub.status.idle":"2024-10-08T02:38:48.851413Z","shell.execute_reply.started":"2024-10-08T02:38:48.845945Z","shell.execute_reply":"2024-10-08T02:38:48.850448Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_vivit_model(model, data_loader, criterion, optimizer, device, num_epochs=10):\n    \"\"\"\n    Trains the Vision Transformer (ViViT) model.\n    \n    Args:\n        model (`nn.Module`): The ViViT model to train.\n        data_loader (`DataLoader`): The PyTorch DataLoader with HandwashingDataset.\n        criterion (`torch.nn.Module`): Loss function (e.g., CrossEntropyLoss).\n        optimizer (`torch.optim.Optimizer`): Optimizer (e.g., Adam, SGD).\n        device (`torch.device`): Device to train on ('cuda' or 'cpu').\n        num_epochs (`int`): Number of epochs to train (default: 10).\n    \n    Returns:\n        model: Trained ViViT model.\n    \"\"\"\n    # Move model to the device (GPU or CPU)\n    model.to(device)\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        running_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n        \n        # Loop over data batches\n        for batch in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            # Get inputs and labels, move them to the device\n            inputs = batch['video'].to(device)  # (batch_size, num_frames, C, H, W)\n            labels = batch['class'].to(device)  # (batch_size,)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(**inputs)  # (batch_size, num_classes)\n            \n            # Calculate the loss\n            loss = criterion(outputs.logits, labels)\n            \n            # Backward pass + optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Track loss and accuracy\n            running_loss += loss.item()\n            preds = outputs.logits.argmax(-1).item()  # Get predicted class\n            correct_predictions += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n        \n        # Calculate average loss and accuracy for the epoch\n        epoch_loss = running_loss / total_samples\n        epoch_acc = correct_predictions / total_samples\n        \n        # Print statistics\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:40:50.687416Z","iopub.execute_input":"2024-10-08T02:40:50.687848Z","iopub.status.idle":"2024-10-08T02:40:50.699094Z","shell.execute_reply.started":"2024-10-08T02:40:50.687804Z","shell.execute_reply":"2024-10-08T02:40:50.697893Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# create the model from the config\nmodel = VivitForVideoClassification(config)\n\n# set the optimizer, criterion, and device\ncriterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available else nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# input the data\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\ndata_loader = HandwashingDataset(data_dir, train_vid_names, image_processor)\n\ntrain_vivit_model(model, data_loader, criterion, optimizer, device, 1)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:40:50.919544Z","iopub.execute_input":"2024-10-08T02:40:50.920182Z","iopub.status.idle":"2024-10-08T02:53:04.504025Z","shell.execute_reply.started":"2024-10-08T02:40:50.920133Z","shell.execute_reply":"2024-10-08T02:53:04.503079Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 240/240 [12:11<00:00,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.7830, Accuracy: 0.8250\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"VivitForVideoClassification(\n  (vivit): VivitModel(\n    (embeddings): VivitEmbeddings(\n      (patch_embeddings): VivitTubeletEmbeddings(\n        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): VivitEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VivitLayer(\n          (attention): VivitAttention(\n            (attention): VivitSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): VivitSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): VivitIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_act_fn): FastGELUActivation()\n          )\n          (output): VivitOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=12, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model, \"cur_model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:54:16.426815Z","iopub.execute_input":"2024-10-08T02:54:16.427216Z","iopub.status.idle":"2024-10-08T02:54:16.902518Z","shell.execute_reply.started":"2024-10-08T02:54:16.427177Z","shell.execute_reply":"2024-10-08T02:54:16.901661Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-10-08T02:54:24.736437Z","iopub.execute_input":"2024-10-08T02:54:24.736832Z","iopub.status.idle":"2024-10-08T02:54:25.802598Z","shell.execute_reply.started":"2024-10-08T02:54:24.736795Z","shell.execute_reply":"2024-10-08T02:54:25.801333Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"cur_model.pkl  transformers\n","output_type":"stream"}]}]}