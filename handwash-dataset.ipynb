{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596d7d6d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-08T04:41:49.249157Z",
     "iopub.status.busy": "2024-10-08T04:41:49.248389Z",
     "iopub.status.idle": "2024-10-08T04:41:49.253521Z",
     "shell.execute_reply": "2024-10-08T04:41:49.252794Z"
    },
    "papermill": {
     "duration": 0.012448,
     "end_time": "2024-10-08T04:41:49.255413",
     "exception": false,
     "start_time": "2024-10-08T04:41:49.242965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25744e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:41:49.263935Z",
     "iopub.status.busy": "2024-10-08T04:41:49.263625Z",
     "iopub.status.idle": "2024-10-08T04:42:43.868167Z",
     "shell.execute_reply": "2024-10-08T04:42:43.867309Z"
    },
    "papermill": {
     "duration": 54.611402,
     "end_time": "2024-10-08T04:42:43.870658",
     "exception": false,
     "start_time": "2024-10-08T04:41:49.259256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\r\n",
      "remote: Enumerating objects: 233241, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (596/596), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (337/337), done.\u001b[K\r\n",
      "remote: Total 233241 (delta 368), reused 385 (delta 212), pack-reused 232645 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (233241/233241), 247.00 MiB | 29.57 MiB/s, done.\r\n",
      "Resolving deltas: 100% (169894/169894), done.\r\n",
      "Collecting av\r\n",
      "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\r\n",
      "Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: av\r\n",
      "Successfully installed av-13.1.0\r\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/huggingface/transformers.git\n",
    "! pip3 install av\n",
    "    \n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import av\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import VivitConfig, VivitModel, VivitImageProcessor, VivitForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3af80b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:42:43.915019Z",
     "iopub.status.busy": "2024-10-08T04:42:43.914405Z",
     "iopub.status.idle": "2024-10-08T04:42:44.012539Z",
     "shell.execute_reply": "2024-10-08T04:42:44.011832Z"
    },
    "papermill": {
     "duration": 0.122045,
     "end_time": "2024-10-08T04:42:44.014852",
     "exception": false,
     "start_time": "2024-10-08T04:42:43.892807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '../input/hand-wash-dataset/HandWashDataset/HandWashDataset'\n",
    "\n",
    "classes = ['Step_1', 'Step_2_Left', 'Step_2_Right', 'Step_3', 'Step_4_Left', 'Step_4_Right', 'Step_5_Left',\n",
    "           'Step_5_Right', 'Step_6_Left', 'Step_6_Right', 'Step_7_Left', 'Step_7_Right']\n",
    "\n",
    "def get_classes_filenames(data_dir, classes):\n",
    "    \"\"\"\n",
    "    Filenames of files in data_dir/classes[i] for each i\n",
    "    Args:\n",
    "        data_dir (`str`): filepath to a directory\n",
    "        classes (`list[str]`): list of subdirectories of data_dir\n",
    "    Return:\n",
    "        filenames (`dict`): dictionary of filenames\n",
    "    \"\"\"\n",
    "    filenames = {}\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        filenames[class_name] = os.listdir(class_dir)\n",
    "    return filenames\n",
    "\n",
    "filenames = get_classes_filenames(data_dir, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07dc75c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:42:44.055106Z",
     "iopub.status.busy": "2024-10-08T04:42:44.054790Z",
     "iopub.status.idle": "2024-10-08T04:42:44.071345Z",
     "shell.execute_reply": "2024-10-08T04:42:44.070165Z"
    },
    "papermill": {
     "duration": 0.039051,
     "end_time": "2024-10-08T04:42:44.073547",
     "exception": false,
     "start_time": "2024-10-08T04:42:44.034496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxilliary functions from Hugging Face\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "label2id = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "id2label = {idx : class_name for idx, class_name in enumerate(classes)}\n",
    "\n",
    "# Class of dataset\n",
    "class HandwashingDataset(Dataset):\n",
    "    \"\"\"Handwashing Dataset.\"\"\"\n",
    "    def __init__(self, file_path, file_names, image_processor, transform=None):\n",
    "        self.labels = []\n",
    "        self.file_paths = []\n",
    "        self.image_processor = image_processor\n",
    "        for label in file_names.keys():\n",
    "            for name in file_names[label]:\n",
    "                full_path = os.path.join(file_path, label, name)\n",
    "                self.file_paths.append(full_path)\n",
    "                self.labels.append(label2id[label])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_container = av.open(self.file_paths[idx])\n",
    "        vid_indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=vid_container.streams.video[0].frames)\n",
    "        vid_read = read_video_pyav(container=vid_container, indices=vid_indices)\n",
    "        vid_input = self.image_processor(list(vid_read), return_tensors=\"pt\")\n",
    "        vid_input['pixel_values'] = vid_input['pixel_values'].squeeze(0)\n",
    "        \n",
    "        return {'video' : vid_input, 'class' : torch.tensor(self.labels[idx], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bd3f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:42:44.123171Z",
     "iopub.status.busy": "2024-10-08T04:42:44.122790Z",
     "iopub.status.idle": "2024-10-08T04:42:44.138823Z",
     "shell.execute_reply": "2024-10-08T04:42:44.137800Z"
    },
    "papermill": {
     "duration": 0.045152,
     "end_time": "2024-10-08T04:42:44.141226",
     "exception": false,
     "start_time": "2024-10-08T04:42:44.096074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(filenames, test_size=0.2, random_state=20):\n",
    "    test = {}\n",
    "    train = {}\n",
    "    for label in filenames.keys():\n",
    "        train_names, test_names = train_test_split(filenames[label], test_size=test_size, random_state=random_state)\n",
    "        test[label] = test_names\n",
    "        train[label] = train_names\n",
    "    return train, test\n",
    "\n",
    "# Preparing the dataset and splititng into training and test datasets\n",
    "train_vid_names, test_vid_names = split_dataset(filenames, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ea8aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:42:44.203397Z",
     "iopub.status.busy": "2024-10-08T04:42:44.203063Z",
     "iopub.status.idle": "2024-10-08T04:42:44.208624Z",
     "shell.execute_reply": "2024-10-08T04:42:44.207753Z"
    },
    "papermill": {
     "duration": 0.034294,
     "end_time": "2024-10-08T04:42:44.210674",
     "exception": false,
     "start_time": "2024-10-08T04:42:44.176380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = VivitConfig()\n",
    "\n",
    "config.num_labels = len(classes)\n",
    "for i in range(len(classes)):\n",
    "    id2label[i] = classes[i]\n",
    "    label2id[classes[i]] = i\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c08fbca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:42:44.250337Z",
     "iopub.status.busy": "2024-10-08T04:42:44.249534Z",
     "iopub.status.idle": "2024-10-08T04:42:44.259165Z",
     "shell.execute_reply": "2024-10-08T04:42:44.258489Z"
    },
    "papermill": {
     "duration": 0.030958,
     "end_time": "2024-10-08T04:42:44.261079",
     "exception": false,
     "start_time": "2024-10-08T04:42:44.230121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_vivit_model(model, data_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the Vision Transformer (ViViT) model.\n",
    "    \n",
    "    Args:\n",
    "        model (`nn.Module`): The ViViT model to train.\n",
    "        data_loader (`DataLoader`): The PyTorch DataLoader with HandwashingDataset.\n",
    "        criterion (`torch.nn.Module`): Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (`torch.optim.Optimizer`): Optimizer (e.g., Adam, SGD).\n",
    "        device (`torch.device`): Device to train on ('cuda' or 'cpu').\n",
    "        num_epochs (`int`): Number of epochs to train (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained ViViT model.\n",
    "    \"\"\"\n",
    "    # Move model to the device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Loop over data batches\n",
    "        for batch in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Get inputs and labels, move them to the device\n",
    "            inputs = batch['video'].to(device)  # (batch_size, num_frames, C, H, W)\n",
    "            labels = batch['class'].to(device)  # (batch_size,)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)  # (batch_size, num_classes)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            # Backward pass + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.logits.argmax(-1).item()  # Get predicted class\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = correct_predictions / total_samples\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c62ab02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T04:42:44.299263Z",
     "iopub.status.busy": "2024-10-08T04:42:44.298972Z",
     "iopub.status.idle": "2024-10-08T08:44:19.160660Z",
     "shell.execute_reply": "2024-10-08T08:44:19.159572Z"
    },
    "papermill": {
     "duration": 14494.883049,
     "end_time": "2024-10-08T08:44:19.162674",
     "exception": false,
     "start_time": "2024-10-08T04:42:44.279625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63198907e5ab4ccb843d98ba6b7a65d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(value)\n",
      "Epoch 1/20: 100%|██████████| 240/240 [12:11<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.4056, Accuracy: 0.0875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 240/240 [12:03<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 2.9530, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 240/240 [12:15<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 2.7745, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 240/240 [12:00<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 2.7056, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 240/240 [11:59<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 2.6296, Accuracy: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 240/240 [11:53<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 2.5944, Accuracy: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 240/240 [12:03<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 2.6050, Accuracy: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 240/240 [12:08<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 2.5716, Accuracy: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 240/240 [12:02<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 2.5503, Accuracy: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 240/240 [11:56<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 2.5598, Accuracy: 0.0375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 240/240 [12:03<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 2.5442, Accuracy: 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 240/240 [12:11<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 2.5329, Accuracy: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 240/240 [12:19<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 2.5127, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 240/240 [12:06<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 2.5130, Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 240/240 [12:12<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 2.5342, Accuracy: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 240/240 [11:47<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 2.5235, Accuracy: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 240/240 [12:04<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 2.5259, Accuracy: 0.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 240/240 [11:58<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 2.5132, Accuracy: 0.0875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 240/240 [12:06<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 2.5130, Accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 240/240 [12:09<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 2.5171, Accuracy: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VivitForVideoClassification(\n",
       "  (vivit): VivitModel(\n",
       "    (embeddings): VivitEmbeddings(\n",
       "      (patch_embeddings): VivitTubeletEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): VivitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VivitLayer(\n",
       "          (attention): VivitAttention(\n",
       "            (attention): VivitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VivitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VivitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_act_fn): FastGELUActivation()\n",
       "          )\n",
       "          (output): VivitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model from the config\n",
    "model = VivitForVideoClassification(config)\n",
    "#model = torch.load(\"cur_model.pkl\", weights_only=False)\n",
    "\n",
    "# set the optimizer, criterion, and device\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available else nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# input the data\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "dataset = HandwashingDataset(data_dir, train_vid_names, image_processor)\n",
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "train_vivit_model(model, data_loader, criterion, optimizer, device, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ccd803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T08:44:19.955502Z",
     "iopub.status.busy": "2024-10-08T08:44:19.955114Z",
     "iopub.status.idle": "2024-10-08T08:44:20.456452Z",
     "shell.execute_reply": "2024-10-08T08:44:20.455537Z"
    },
    "papermill": {
     "duration": 0.899102,
     "end_time": "2024-10-08T08:44:20.458679",
     "exception": false,
     "start_time": "2024-10-08T08:44:19.559577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"cur_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85646f54",
   "metadata": {
    "papermill": {
     "duration": 0.401442,
     "end_time": "2024-10-08T08:44:21.304033",
     "exception": false,
     "start_time": "2024-10-08T08:44:20.902591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"cur_model.pkl\"> Download File </a>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 416449,
     "sourceId": 1093962,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14558.845475,
   "end_time": "2024-10-08T08:44:25.323169",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-08T04:41:46.477694",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11a969a129124ffeaf55025fec579844": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "199c5b3fd7ac42a3b5133f0e372486ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_546fabd6d3714b4b92a317100ef9d6da",
       "placeholder": "​",
       "style": "IPY_MODEL_11a969a129124ffeaf55025fec579844",
       "value": "preprocessor_config.json: 100%"
      }
     },
     "2fbd79bcd4cb496db87fc38d60991d15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4c9cb2f9e57049368cea7e87b1e3c4dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b31560aa31a940f0b8b236d988540f85",
       "max": 401.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5dfe89f336b84129bc78fdd95cd67440",
       "value": 401.0
      }
     },
     "546fabd6d3714b4b92a317100ef9d6da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5dfe89f336b84129bc78fdd95cd67440": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "63198907e5ab4ccb843d98ba6b7a65d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_199c5b3fd7ac42a3b5133f0e372486ad",
        "IPY_MODEL_4c9cb2f9e57049368cea7e87b1e3c4dc",
        "IPY_MODEL_cd783f46d9b046faa9df55b58842a321"
       ],
       "layout": "IPY_MODEL_98161a4aa0d448e39a459661035423de"
      }
     },
     "6bdb8e398a7a41ef94ac950d81025f4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98161a4aa0d448e39a459661035423de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b31560aa31a940f0b8b236d988540f85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd783f46d9b046faa9df55b58842a321": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6bdb8e398a7a41ef94ac950d81025f4a",
       "placeholder": "​",
       "style": "IPY_MODEL_2fbd79bcd4cb496db87fc38d60991d15",
       "value": " 401/401 [00:00&lt;00:00, 31.1kB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
