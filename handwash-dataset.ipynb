{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1093962,"sourceType":"datasetVersion","datasetId":416449}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-07T20:41:07.418973Z","iopub.execute_input":"2024-10-07T20:41:07.420079Z","iopub.status.idle":"2024-10-07T20:41:07.426405Z","shell.execute_reply.started":"2024-10-07T20:41:07.420030Z","shell.execute_reply":"2024-10-07T20:41:07.425191Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Start Monday Oct 7th 2024\nimport os\nprint(os.getcwd())\n\n! git clone https://github.com/huggingface/transformers.git\n! ls /kaggle/working/transformers/src","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:41:07.432213Z","iopub.execute_input":"2024-10-07T20:41:07.433334Z","iopub.status.idle":"2024-10-07T20:41:31.595578Z","shell.execute_reply.started":"2024-10-07T20:41:07.433272Z","shell.execute_reply":"2024-10-07T20:41:31.594098Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'transformers'...\nremote: Enumerating objects: 233169, done.\u001b[K\nremote: Counting objects: 100% (530/530), done.\u001b[K\nremote: Compressing objects: 100% (271/271), done.\u001b[K\nremote: Total 233169 (delta 318), reused 375 (delta 212), pack-reused 232639 (from 1)\u001b[K\nReceiving objects: 100% (233169/233169), 246.94 MiB | 25.45 MiB/s, done.\nResolving deltas: 100% (169859/169859), done.\ntransformers\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install AvPy\n! pip3 install av\n\n\nimport random\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport numpy as np\nimport shutil # for moving files around\n\nfrom sklearn.model_selection import train_test_split\n\n# Import relevant packages\nimport av\nfrom transformers import VivitConfig, VivitModel, VivitImageProcessor, VivitForVideoClassification\nfrom huggingface_hub import hf_hub_download","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:41:31.598476Z","iopub.execute_input":"2024-10-07T20:41:31.599024Z","iopub.status.idle":"2024-10-07T20:42:09.756988Z","shell.execute_reply.started":"2024-10-07T20:41:31.598958Z","shell.execute_reply":"2024-10-07T20:42:09.755833Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-13.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import various libraries, take model and split into classes, move them into the working directory\n\ndata_dir = '../input/hand-wash-dataset/HandWashDataset/HandWashDataset'\n\nclasses = ['Step_1', 'Step_2_Left', 'Step_2_Right', 'Step_3', 'Step_4_Left', 'Step_4_Right', 'Step_5_Left',\n           'Step_5_Right', 'Step_6_Left', 'Step_6_Right', 'Step_7_Left', 'Step_7_Right']\n\ndef split_dataset(data_dir, classes, test_size=0.2, random_state=20):\n    video_lengths = []\n    for class_name in classes:\n        class_dir = os.path.join(data_dir, class_name)\n        videos = os.listdir(class_dir)\n        for video_file in videos:\n            video_path = os.path.join(class_dir, video_file)\n            video_lengths.append((class_name, video_path))\n            \n    train_vid, test_vid = train_test_split(video_lengths, test_size=test_size, random_state=random_state)\n    return train_vid, test_vid\n\ndef copy_videos_to_folders(videos, output_dir, set_name):\n    for class_name, video_path in videos:\n        destination_dir = os.path.join(output_dir, set_name, class_name)\n        if not os.path.exists(destination_dir):\n            os.makedirs(destination_dir)\n        filename = os.path.basename(video_path)\n        dest_file = os.path.join(destination_dir, filename)\n        shutil.copy(video_path, dest_file)\n\n# Preparing the dataset and splititng into training and test datasets\ntrain_videos, test_videos = split_dataset(data_dir, classes)\n\n# output directory\noutput_dir = '/kaggle/working'\ncopy_videos_to_folders(train_videos, output_dir, 'train')\ncopy_videos_to_folders(test_videos, output_dir, 'test')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:42:09.758529Z","iopub.execute_input":"2024-10-07T20:42:09.759218Z","iopub.status.idle":"2024-10-07T20:42:27.192553Z","shell.execute_reply.started":"2024-10-07T20:42:09.759174Z","shell.execute_reply":"2024-10-07T20:42:27.191325Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Auxilliary functions from Hugging Face\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:42:27.195115Z","iopub.execute_input":"2024-10-07T20:42:27.195559Z","iopub.status.idle":"2024-10-07T20:42:27.206324Z","shell.execute_reply.started":"2024-10-07T20:42:27.195514Z","shell.execute_reply":"2024-10-07T20:42:27.205007Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom torch.utils.data import Dataset, DataLoader\n\n# Class of dataset (Needs a lot of preprocessing)\nclass HandwashingDataset(Dataset):\n    \"\"\"Handwashing Dataset.\"\"\"\n    def __init__(self, videos, labels, transform=None):\n        assert(len(videos) == len(labels))\n        self.videos = videos\n        self.classes = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        return {'video' : videos[idx], 'class' : labels[idx]}\n\ndef make_dataset(data_dir, classes, image_processor):\n    # data_dir refers to the directory from 'working/kaggle' (typically 'test' or 'train')\n    # assumes that at 'working/kaggle/data_dir' there is a directory for each class in classes\n    videos = []\n    labels = []\n    for i in range(len(classes)):\n        print(f\"\\rDownloading from {data_dir}/{classes[i]}\")\n        \n        # Navigate to the class directory and list out the files in this directory\n        class_dir = os.path.join(data_dir, classes[i])\n        video_names = os.listdir(class_dir)\n        for j in range(len(video_names)):\n            print(f\"\\r Importing {i+1} out of {len(video_names)}\", end=\"\")\n            sys.stdout.flush()\n            \n            vid_title = video_names[j] # Get title\n            path = os.path.join(class_dir, vid_title) # Navigate to title\n            vid_container = av.open(path) # Open video at title\n            vid_indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=vid_container.streams.video[0].frames) # Get random indices\n            vid_read = read_video_pyav(container=vid_container, indices=vid_indices) # read the video\n            vid_input = image_processor(list(vid_read), return_tensors=\"pt\") # preprocess the video\n            videos.append(vid_input) # add the video to the list\n            labels.append(j) # add the label\n            \n    print(\"\\nDone\")\n    return (videos, labels)\n\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\nvideos, labels = make_dataset('train', classes, image_processor)\ndata_loader = HandwashingDataset(videos, labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:42:27.207953Z","iopub.execute_input":"2024-10-07T20:42:27.208443Z","iopub.status.idle":"2024-10-07T20:53:44.878646Z","shell.execute_reply.started":"2024-10-07T20:42:27.208389Z","shell.execute_reply":"2024-10-07T20:53:44.876064Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acaf979bb65a47939c5a3e092c9f1721"}},"metadata":{}},{"name":"stdout","text":"Downloading from train/Step_1\n Importing 1 out of 20","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.tensor(value)\n","output_type":"stream"},{"name":"stdout","text":"Downloading from train/Step_2_Left\nDownloading from train/Step_2_Right\nDownloading from train/Step_3\nDownloading from train/Step_4_Left\nDownloading from train/Step_4_Right\nDownloading from train/Step_5_Left\nDownloading from train/Step_5_Right\nDownloading from train/Step_6_Left\nDownloading from train/Step_6_Right\nDownloading from train/Step_7_Left\nDownloading from train/Step_7_Right\n Importing 12 out of 18\nDone\n","output_type":"stream"}]},{"cell_type":"code","source":"type(videos[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:53:44.881383Z","iopub.execute_input":"2024-10-07T20:53:44.881886Z","iopub.status.idle":"2024-10-07T20:53:44.895542Z","shell.execute_reply.started":"2024-10-07T20:53:44.881835Z","shell.execute_reply":"2024-10-07T20:53:44.894308Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"transformers.image_processing_base.BatchFeature"},"metadata":{}}]},{"cell_type":"code","source":"# set configuration and initalize the desired model\nimport torch\n\nconfig = VivitConfig()\nconfig.num_labels = len(classes)\n\nmodel = VivitForVideoClassification(config)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available else nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:57:07.364469Z","iopub.execute_input":"2024-10-07T20:57:07.365259Z","iopub.status.idle":"2024-10-07T20:57:08.855534Z","shell.execute_reply.started":"2024-10-07T20:57:07.365168Z","shell.execute_reply":"2024-10-07T20:57:08.854508Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"outputs = model(**videos[0])\nlogits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nprint(outputs)\nprint(logits)\nprint(predicted_label)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T21:07:42.751061Z","iopub.execute_input":"2024-10-07T21:07:42.752289Z","iopub.status.idle":"2024-10-07T21:08:05.100382Z","shell.execute_reply.started":"2024-10-07T21:07:42.752208Z","shell.execute_reply":"2024-10-07T21:08:05.099056Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"ImageClassifierOutput(loss=None, logits=tensor([[ 0.5762, -0.1448,  0.1088,  1.0504, -0.4075, -0.6358,  0.1499,  0.9902,\n          0.5092,  0.1598,  0.0804,  1.0279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\ntensor([[ 0.5762, -0.1448,  0.1088,  1.0504, -0.4075, -0.6358,  0.1499,  0.9902,\n          0.5092,  0.1598,  0.0804,  1.0279]], grad_fn=<AddmmBackward0>)\n3\n","output_type":"stream"}]},{"cell_type":"code","source":"desired = torch.from_numpy(np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n\ncriterion(logits, desired)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T21:13:34.365305Z","iopub.execute_input":"2024-10-07T21:13:34.365783Z","iopub.status.idle":"2024-10-07T21:13:34.376442Z","shell.execute_reply.started":"2024-10-07T21:13:34.365739Z","shell.execute_reply":"2024-10-07T21:13:34.375338Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor(2.3352, dtype=torch.float64, grad_fn=<DivBackward1>)"},"metadata":{}}]},{"cell_type":"code","source":"def train_net(n_epochs):\n    \n    model.train()\n    for epoch in range(n_epochs):\n        running_loss = 0.0\n        \n        for batch_i, data in enumerate(data_loader):\n            video = data['video'].to(device)\n            label = data['class'].to(device)\n            \n            outputs = model(video)\n            loss = criterion(outputs.view(-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}